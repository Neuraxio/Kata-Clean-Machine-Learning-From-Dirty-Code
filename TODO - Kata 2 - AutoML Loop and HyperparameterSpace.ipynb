{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kata 2: AutoML Loop & HyperparameterSpace\n",
    "\n",
    "Let's now use the solution of the Kata 1 and try to do AutoML from there. \n",
    "\n",
    "A call to the `AutoML` class is already made at the end of this notebook, after which the best model found automatically using the accuracy on the validation dataset is retrained on the train dataset and the validation dataset to be tested on the test dataset. We use a simple train/val split for the validation.\n",
    "\n",
    "## The task\n",
    "\n",
    "Your goal is then to add more choices to try in the pipeline (and with more hyperparameters per choice) to make it really good. \n",
    "\n",
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "\n",
    "def download_import(filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        # Downloading like that is needed because of Colab operating from a Google Drive folder that is only \"shared with you\".\n",
    "        url = 'https://raw.githubusercontent.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code/master/{}'.format(filename)\n",
    "        f.write(urllib.request.urlopen(url).read())\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    download_import(\"data_loading.py\")\n",
    "    !mkdir data;\n",
    "    download_import(\"data/download_dataset.py\")\n",
    "    print(\"Downloaded .py files: dataset loaders.\")\n",
    "except:\n",
    "    print(\"No dynamic .py file download needed: not in a Colab.\")\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "!pwd && ls\n",
    "os.chdir(DATA_PATH)\n",
    "!pwd && ls\n",
    "!python download_dataset.py\n",
    "!pwd && ls\n",
    "os.chdir(\"..\")\n",
    "!pwd && ls\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install neuraxle if needed:\n",
    "try:\n",
    "    import neuraxle\n",
    "    assert neuraxle.__version__ == '0.3.4'\n",
    "except:\n",
    "    !pip install neuraxle==0.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally load dataset!\n",
    "from data_loading import load_all_data\n",
    "X_train, y_train, X_test, y_test = load_all_data()\n",
    "print(\"Dataset loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's reuse our pipeline steps as we should have created them in Kata 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neuraxle.base import BaseStep, NonFittableMixin\n",
    "\n",
    "\n",
    "class NumpyFFT(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Featurize time series data with FFT.\n",
    "\n",
    "        :param data_inputs: time series data of 3D shape: [batch_size, time_steps, sensors_readings]\n",
    "        :return: featurized data is of 2D shape: [batch_size, n_features]\n",
    "        \"\"\"\n",
    "        transformed_data = np.fft.rfft(data_inputs, axis=-2)\n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "class FFTPeakBinWithValue(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will compute peak fft bins (int), and their magnitudes' value (float), to concatenate them.\n",
    "\n",
    "        :param data_inputs: real magnitudes of an fft. It could be of shape [batch_size, bins, features].\n",
    "        :return: Two arrays without bins concatenated on feature axis. Shape: [batch_size, 2 * features]\n",
    "        \"\"\"\n",
    "        time_bins_axis = -2\n",
    "        peak_bin = np.argmax(data_inputs, axis=time_bins_axis)\n",
    "        peak_bin_val = np.max(data_inputs, axis=time_bins_axis)\n",
    "\n",
    "        # Notice that here another FeatureUnion could have been used with a joiner:\n",
    "        transformed = np.concatenate([peak_bin, peak_bin_val], axis=-1)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "\n",
    "class NumpyAbs(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.abs(data_inputs)\n",
    "\n",
    "\n",
    "class NumpyMean(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a mean.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.mean(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyRavel(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        if data_inputs is not None:\n",
    "            data_inputs = data_inputs if isinstance(data_inputs, np.ndarray) else np.array(data_inputs)\n",
    "            return data_inputs.ravel()\n",
    "        return data_inputs\n",
    "\n",
    "\n",
    "class NumpyMedian(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a median.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.median(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMin(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a min.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.min(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMax(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.max(data_inputs, axis=-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some classifiers here and their hyperparam space\n",
    "\n",
    "You'll want to combine a few classifiers here in the pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.hyperparams.distributions import Choice, Boolean\n",
    "from neuraxle.hyperparams.distributions import RandInt, LogUniform\n",
    "from neuraxle.hyperparams.space import HyperparameterSpace\n",
    "from neuraxle.pipeline import Pipeline\n",
    "from neuraxle.steps.output_handlers import OutputTransformerWrapper\n",
    "from neuraxle.steps.sklearn import SKLearnWrapper\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "decision_tree_classifier = SKLearnWrapper(\n",
    "    DecisionTreeClassifier(), HyperparameterSpace({\n",
    "        'criterion': Choice(['gini', 'entropy']),\n",
    "        # 'splitter': Choice([...]),  # TODO... see documentation of the `DecisionTreeClassifier` for\n",
    "        #                               this 'splitter' argument's possible string values as choice.\n",
    "        'min_samples_leaf': RandInt(2, 5),\n",
    "        # 'min_samples_split': ...,  # TODO\n",
    "    }))\n",
    "\n",
    "logistic_regression = Pipeline([\n",
    "    # TODO: you might want to read this...:\n",
    "    #       https://stackoverflow.com/a/60651754/2476920\n",
    "    OutputTransformerWrapper(NumpyRavel()),\n",
    "    SKLearnWrapper(LogisticRegression(), HyperparameterSpace({\n",
    "        'C': LogUniform(0.01, 10.0),\n",
    "        'fit_intercept': Boolean(),\n",
    "        # 'dual': ...,  # TODO\n",
    "        # 'penalty': ...,  # TODO\n",
    "        'max_iter': RandInt(20, 200)\n",
    "    }))\n",
    "]).set_name('LogisticRegression')\n",
    "\n",
    "# TODO, fill those ones as well: \n",
    "# extra_tree_classifier = ... ExtraTreeClassifier() ...\n",
    "# ridge_classifier = ... RidgeClassifier() ...\n",
    "# random_forest_classifier = ... RandomForestClassifier() ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add your classifiers to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.base import Identity\n",
    "from neuraxle.steps.flow import TrainOnlyWrapper, ChooseOneStepOf\n",
    "from neuraxle.steps.numpy import NumpyConcatenateInnerFeatures, NumpyShapePrinter, NumpyFlattenDatum\n",
    "from neuraxle.union import FeatureUnion\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Input shape before feature union\")),\n",
    "    FeatureUnion([\n",
    "        Pipeline([\n",
    "            NumpyFFT(),\n",
    "            NumpyAbs(),\n",
    "            FeatureUnion([\n",
    "                NumpyFlattenDatum(),  # Reshape from 3D to flat 2D: flattening data except on batch size\n",
    "                FFTPeakBinWithValue()  # Extract 2D features from the 3D FFT bins\n",
    "            ], joiner=NumpyConcatenateInnerFeatures())\n",
    "        ]),\n",
    "        NumpyMean(),\n",
    "        NumpyMedian(),\n",
    "        NumpyMin(),\n",
    "        NumpyMax()\n",
    "    ], joiner=NumpyConcatenateInnerFeatures()),\n",
    "    # TODO, optional: Add some feature selection right here for the motivated ones:\n",
    "    #      https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "    # TODO, optional: Add normalization right here (if using other classifiers)\n",
    "    #      https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Shape after feature union, before classification\")),\n",
    "    # Shape: [batch_size, remade_features]\n",
    "    ChooseOneStepOf([\n",
    "        decision_tree_classifier,\n",
    "        # extra_tree_classifier,  # TODO\n",
    "        # ridge_classifier,  # TODO\n",
    "        logistic_regression,\n",
    "        # random_forest_classifier  # TODO\n",
    "    ]),\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Shape at output after classification\")),\n",
    "    # Shape: [batch_size]\n",
    "    Identity()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally do AutoML! Launch the main AutoML optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "\n",
    "# Clear cache if we've already ran the AutoML to start fresh:\n",
    "cache_folder = 'cache'\n",
    "if os.path.exists(cache_folder):\n",
    "    shutil.rmtree(cache_folder)\n",
    "os.makedirs(cache_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.metaopt.auto_ml import AutoML, InMemoryHyperparamsRepository, validation_splitter, \\\n",
    "    RandomSearchHyperparameterSelectionStrategy\n",
    "from neuraxle.metaopt.callbacks import ScoringCallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "auto_ml = AutoML(\n",
    "    pipeline=pipeline,\n",
    "    hyperparams_optimizer=RandomSearchHyperparameterSelectionStrategy(),\n",
    "    validation_split_function=validation_splitter(test_size=0.20),\n",
    "    scoring_callback=ScoringCallback(accuracy_score, higher_score_is_better=True),\n",
    "    n_trials=7,\n",
    "    epochs=1,\n",
    "    hyperparams_repository=InMemoryHyperparamsRepository(cache_folder=cache_folder),\n",
    "    refit_trial=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do AutoML by selecting on validation data, and get best model refitted on all train and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml = auto_ml.fit(X_train, y_train)\n",
    "best_pipeline = auto_ml.get_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test data and score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Test accuracy score:\", accuracy)\n",
    "assert accuracy > 0.85, \"Try again harder!\"\n",
    "# It's getting good on this dataset if you're over 92%. The current code is able to do this.\n",
    "# Getting to 94% is a very hard task on this dataset.Â "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the end!\n",
    "\n",
    "Congratulations. You won.\n",
    "\n",
    "## Recommended additional readings and learning resources: \n",
    "\n",
    "- For more info on clean machine learning, you may want to read [How to Code Neat Machine Learning Pipelines](https://www.neuraxio.com/en/blog/neuraxle/2019/10/26/neat-machine-learning-pipelines.html).\n",
    "- For reaching higher performances, you could use a [LSTM Recurrent Neural Network](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) and refactoring it into a neat pipeline as you've created here, now by [using TensorFlow in your ML pipeline](https://github.com/Neuraxio/Neuraxle-TensorFlow).\n",
    "- You may as well want to request [more training and coaching for your ML or time series processing projects](https://www.neuraxio.com/en/time-series-solution) from us if you need.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "git_kata_ml",
   "language": "python",
   "name": "git_kata_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
